{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600190884780",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embedding_glove = GloVe(name='6B', dim=50)"
   ]
  },
  {
   "source": [
    "# Self-attention\n",
    "Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let’s call the input vectors 𝐱1,𝐱2,…𝐱t and the corresponding output vectors 𝐲1,𝐲2,…,𝐲t. The vectors all have dimension k.\n",
    "\n",
    "To apply self-attention, we simply assign each word t in our vocabulary an embedding vector 𝐯t (the values of which we’ll learn). This is what’s known as an embedding layer in sequence modeling. It turns the word sequence the,cat,walks,on,the,street into the vector sequence\n",
    "\n",
    "𝐯-the,𝐯-cat,𝐯-walks,𝐯-on,𝐯-the,𝐯-street.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([6, 50])\ntorch.Size([1, 6, 50])\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "X = torch.stack((embedding_glove['the'], embedding_glove['cat'], embedding_glove['walks'], embedding_glove['on'], embedding_glove['the'], embedding_glove['street']))\n",
    "print(X.shape)\n",
    "X = X.reshape(1, X.shape[0], X.shape[1])\n",
    "print(X.shape)"
   ]
  },
  {
   "source": [
    "To produce output vector 𝐲i, the self attention operation simply takes a weighted average over all the input vectors\n",
    "\n",
    "$$𝐲_{i}= \\sum_{j}w_{ij}𝐱_{j}$$\n",
    "Where j indexes over the whole sequence and the weights sum to one over all j. The weight wij is not a parameter, as in a normal neural net, but it is derived from a function over 𝐱i and 𝐱j. The simplest option for this function is the dot product:\n",
    "\n",
    "$$w^′_{ij}=𝐱_{i}^T𝐱_{j}$$\n",
    "Note that 𝐱i is the input vector at the same position as the current output vector 𝐲i. For the next output vector, we get an entirely new series of dot products, and a different weighted sum.\n",
    "![Image](http://peterbloem.nl/files/transformers/self-attention.svg)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weights = torch.bmm(X, X.transpose(1, 2))\n",
    "#   torch.bmm is a batched matrix multiplication. It \n",
    "#   applies matrix multiplication over batches of \n",
    "#   matrices."
   ]
  },
  {
   "source": [
    "The dot product gives us a value anywhere between negative and positive infinity, so we apply a softmax to map the values to $[0,1]$ and to ensure that they sum to 1 over the whole sequence:\n",
    "\n",
    "$$w_{ij}= \\frac{exp(w^′_{ij})}{∑_{j}exp(w^′_{ij})}$$\n",
    "And that’s the basic operation of self attention.\n",
    "To turn the raw weights $w^′_{ij}$ into positive values that sum to one, we apply a row-wise softmax:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = F.softmax(raw_weights, dim=2)"
   ]
  },
  {
   "source": [
    "To compute the output sequence, we just multiply the weight matrix by 𝐗. This results in a batch of output matrices 𝐘 of size (b, t, k) whose rows are weighted sums over the rows of 𝐗.\n",
    "$$𝐲_{i}=∑_{j}w_{ij}𝐱_{j}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.bmm(weights, X)"
   ]
  },
  {
   "source": [
    "Y is the output-vector with self-attention."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}