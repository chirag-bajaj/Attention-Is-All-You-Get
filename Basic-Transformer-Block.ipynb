{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600341420219",
   "display_name": "Python 3.7.7 64-bit ('py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embedding_glove = GloVe(name='6B', dim=50)"
   ]
  },
  {
   "source": [
    "# Basic Transformer Block\n",
    "### Contents:\n",
    "A self attention layer, layer normalization, a feed forward layer (a single MLP applied independently to each vector), and another layer normalization. Residual connections are added around both, before the normalization.\n",
    "\n",
    "![image](http://peterbloem.nl/files/transformers/transformer-block.svg)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([6, 50])\ntorch.Size([1, 6, 50])\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "X = torch.stack((embedding_glove['the'], embedding_glove['cat'], embedding_glove['walks'], embedding_glove['on'], embedding_glove['the'], embedding_glove['street']))\n",
    "print(X.shape)\n",
    "X = X.reshape(1, X.shape[0], X.shape[1])\n",
    "print(X.shape)"
   ]
  },
  {
   "source": [
    "## Self-Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "num_heads = 8\n",
    "num_words = X.shape[1]\n",
    "num_dim = X.shape[2]\n",
    "queries = nn.Linear(num_dim, num_heads*num_dim, bias=False)\n",
    "keys = nn.Linear(num_dim, num_heads*num_dim, bias=False)\n",
    "values = nn.Linear(num_dim, num_heads*num_dim, bias=False)\n",
    "unify_heads = nn.Linear(num_heads*num_dim, num_dim, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries(X).view(1, num_words, num_heads, num_dim)\n",
    "queries = queries.transpose(1, 2).contiguous().view(1*num_heads, num_words, num_dim)\n",
    "keys = keys(X).view(1, num_words, num_heads, num_dim)\n",
    "keys = keys.transpose(1, 2).contiguous().view(1*num_heads, num_words, num_dim)\n",
    "values = values(X).view(1, num_words, num_heads, num_dim)\n",
    "values = values.transpose(1, 2).contiguous().view(1*num_heads, num_words, num_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries/(num_dim**(1/4))\n",
    "keys = keys/(num_dim**(1/4))\n",
    "\n",
    "raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "weights = F.softmax(raw_weights, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 8, 6, 50])\ntorch.Size([1, 6, 50])\n"
    }
   ],
   "source": [
    "out = torch.bmm(weights, values).view(1, num_heads, num_words, num_dim)\n",
    "print(out.shape)\n",
    "out = out.transpose(1, 2).contiguous().view(1, num_words, num_heads*num_dim)\n",
    "Y = unify_heads(out)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initializations\n",
    "norm1 = nn.LayerNorm(num_dim)\n",
    "layer1 = nn.Linear(num_dim, 4*num_dim)\n",
    "relu = nn.ReLU()\n",
    "layer2 = nn.Linear(4*num_dim, num_dim)\n",
    "norm2 = nn.LayerNorm(num_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = norm1(Y+X)\n",
    "out2 = layer1(out1)\n",
    "out3 = relu(out2)\n",
    "out4 = layer2(out3)\n",
    "final = norm2(out1+out4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 6, 50])\n"
    }
   ],
   "source": [
    "print(final.shape)"
   ]
  }
 ]
}