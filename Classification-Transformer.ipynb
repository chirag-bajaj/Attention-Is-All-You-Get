{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600429288032",
   "display_name": "Python 3.7.7 64-bit ('py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embedding_glove = GloVe(name='6B', dim=50)"
   ]
  },
  {
   "source": [
    "# Self-Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, num_dim):\n",
    "        super.__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "        self.toqueries = nn.Linear(self.num_dim, self.num_heads*self.num_dim, bias=False)\n",
    "        self.tokeys = nn.Linear(self.num_dim, self.num_heads*self.num_dim, bias=False)\n",
    "        self.tovalues = nn.Linear(self.num_dim, self.num_heads*self.num_dim, bias=False)\n",
    "        self.unify_heads = nn.Linear(self.num_heads*self.num_dim, self.num_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_batch, num_words, num_dim = x.shape\n",
    "        num_heads = self.num_heads\n",
    "\n",
    "        queries = self.toqueries(x).view(num_batch, num_words, num_heads, num_dim)\n",
    "        keys = self.tokeys(x).view(num_batch, num_words, num_heads, num_dim)\n",
    "        values = self.tovalues(x).view(num_batch, num_words, num_heads, num_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2).contiguous().view(num_batch*num_heads, num_words, num_dim)\n",
    "        keys = keys.transpose(1, 2).contiguous().view(num_batch*num_heads, num_words, num_dim)\n",
    "        values = values.transpose(1, 2).contiguous().view(num_batch*num_heads, num_words, num_dim)\n",
    "\n",
    "        queries = queries/(num_dim**(1/4))\n",
    "        keys = keys/(num_dim**(1/4))\n",
    "\n",
    "        raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        weights = torch.softmax(raw_weights, dim=2)\n",
    "\n",
    "        out = torch.bmm(weights, values).view(num_batch, num_heads, num_words, num_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(num_batch, num_words, num_heads*num_dim)\n",
    "\n",
    "        out = self.unify_heads(out)\n",
    "        return out"
   ]
  },
  {
   "source": [
    "# Transformer Block"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_dim):\n",
    "        super.__init__()\n",
    "        \n",
    "        self.num_heads = 8\n",
    "        self.num_dim = num_dim\n",
    "        self.sa = SelfAttention(self.num_heads, self.num_dim)\n",
    "        self.norm1 = nn.LayerNorm(self.num_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "                        nn.Linear(self.num_dim, 4*self.num_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(4*self.num_dim, self.num_dim)\n",
    "                        )\n",
    "        self.norm2 = nn.LayerNorm(self.num_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = sa(x)\n",
    "        out2 = norm1(out1+x)\n",
    "        out3 = mlp(out2)\n",
    "        final = norm2(out3+out2)\n",
    "        return final"
   ]
  },
  {
   "source": [
    "# Classification Transformer\n",
    "![image](http://peterbloem.nl/files/transformers/classifier.svg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# IMDB data pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Home Room was a great movie if you've ever had drama in your life. It keeps you wanting to see more. Wondering what the secret Alicia is hiding. I think I watched that movie 6 times in a row and never lost interest. Plus I usually don't cry over movies but this one made me cry each time. I wish I could find more movies like that one. All in All I thought it was a great movie. The more you watch of it the more you become part of it. The very end is the part that really got me when she cried when getting her diploma, because it had her daughter's name on it. My heart felt as if it had shattered just then. And how her new friend came to comfort her when she hadn't gotten hers yet. I loved it so much."
    }
   ],
   "source": [
    "!head 'aclImdb/train/pos/45_10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "trainposfiles = [join('aclImdb/train/pos/', f) for f in listdir('aclImdb/train/pos/') if isfile(join('aclImdb/train/pos/', f))]\n",
    "trainnegfiles = [join('aclImdb/train/neg/', f) for f in listdir('aclImdb/train/neg/') if isfile(join('aclImdb/train/neg/', f))]\n",
    "testposfiles = [join('aclImdb/test/pos/', f) for f in listdir('aclImdb/test/pos/') if isfile(join('aclImdb/test/pos/', f))]\n",
    "testnegfiles = [join('aclImdb/test/neg/', f) for f in listdir('aclImdb/test/neg/') if isfile(join('aclImdb/test/neg/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for x in trainposfiles:\n",
    "    f = open(x, \"r\")\n",
    "    train.append({\"review\": f.read(), \"sentiment\": 1})\n",
    "    f.close()\n",
    "for x in trainnegfiles:\n",
    "    f = open(x, \"r\")\n",
    "    train.append({\"review\": f.read(), \"sentiment\": 0})\n",
    "    f.close()\n",
    "for x in testposfiles:\n",
    "    f = open(x, \"r\")\n",
    "    test.append({\"review\": f.read(), \"sentiment\": 1})\n",
    "    f.close()\n",
    "for x in testnegfiles:\n",
    "    f = open(x, \"r\")\n",
    "    test.append({\"review\": f.read(), \"sentiment\": 0})\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(train)\n",
    "df2 = pd.DataFrame(test)\n",
    "df1.to_csv('train.csv', index=False)\n",
    "df2.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"train.json\", \"w\") as trainfile:  \n",
    "    json.dump(train, trainfile) \n",
    "\n",
    "with open(\"test.json\", \"w\") as testfile:  \n",
    "    json.dump(test, testfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "def tokenize(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "review = Field(sequential=True, tokenize=tokenize, use_vocab=True, lower=True, batch_first=True)\n",
    "sentiment = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\"review\": (\"review\", review), \"sentiment\": (\"sentiment\", sentiment)}\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='/home/chirag_17bit012/Attention-Is-All-You-Get/data',\n",
    "    format='csv',\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    fields=[('review', review), ('sentiment', sentiment)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review.build_vocab(train_data, vectors=\"glove.6B.50d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=128,\n",
    "    sort_key=lambda x: len(x.Text),\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([128, 1051])\ntorch.Size([128, 1194])\ntorch.Size([128, 1088])\ntorch.Size([128, 1212])\ntorch.Size([128, 1081])\ntorch.Size([128, 994])\ntorch.Size([128, 1154])\ntorch.Size([128, 1022])\ntorch.Size([128, 1079])\ntorch.Size([128, 1109])\n"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for b in train_iterator:\n",
    "    cnt+=1\n",
    "    print(b.review.size())\n",
    "    if cnt==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.review.shape"
   ]
  }
 ]
}